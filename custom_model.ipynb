{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Enable mixed precision for faster GPU training and lower memory usage\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Distributed strategy (multi-GPU) if available\n",
    "tf_strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"GPUs detected: {tf_strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = 'data/'\n",
    "IMAGE_SIZE = 224\n",
    "GLOBAL_BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "EPOCHS_HEAD = 20\n",
    "EPOCHS_FINE = 10\n",
    "SEED = 42\n",
    "\n",
    "# 1. Load file paths and labels\n",
    "class_names = sorted(os.listdir(DATA_DIR))\n",
    "paths, labels = [], []\n",
    "for idx, cls in enumerate(class_names):\n",
    "    cls_dir = os.path.join(DATA_DIR, cls)\n",
    "    if os.path.isdir(cls_dir):\n",
    "        cls_paths = glob(os.path.join(cls_dir, '*.*'))\n",
    "        paths += cls_paths\n",
    "        labels += [idx] * len(cls_paths)\n",
    "paths = np.array(paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Shuffle and split\n",
    "i = np.random.RandomState(SEED).permutation(len(paths))\n",
    "paths, labels = paths[i], labels[i]\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    paths, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "# 2. Preprocessing function\n",
    "def preprocess_image(path, label):\n",
    "    img = cv2.imread(path.numpy().decode('utf-8'))\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    eq = cv2.equalizeHist(gray)\n",
    "    blur = cv2.GaussianBlur(eq, (3,3), 0)\n",
    "    _, binar = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    closed = cv2.morphologyEx(binar, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = closed.astype(np.float32) / 255.0\n",
    "\n",
    "    mask3 = np.stack([mask]*3, axis=-1)\n",
    "    masked = (rgb * mask3).astype(np.uint8)\n",
    "\n",
    "    # Normalize for custom CNN\n",
    "    masked = masked.astype(np.float32) / 255.0\n",
    "    return masked, label\n",
    "\n",
    "@tf.function\n",
    "def tf_preprocess(path, label):\n",
    "    img, lbl = tf.py_function(preprocess_image, [path, label], [tf.float32, tf.int64])\n",
    "    img.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    lbl.set_shape([])\n",
    "    return img, lbl\n",
    "\n",
    "# 3. Dataset builder\n",
    "with tf_strategy.scope():\n",
    "    def make_dataset(paths, labels, training=True):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "        if training:\n",
    "            ds = ds.shuffle(1000, seed=SEED)\n",
    "        ds = ds.map(tf_preprocess, num_parallel_calls=AUTO)\n",
    "        if training:\n",
    "            ds = ds.repeat()\n",
    "        ds = ds.batch(GLOBAL_BATCH_SIZE)\n",
    "        ds = ds.prefetch(AUTO)\n",
    "        return ds\n",
    "\n",
    "    train_ds = make_dataset(train_paths, train_labels, training=True)\n",
    "    val_ds = make_dataset(test_paths, test_labels, training=False)\n",
    "\n",
    "    # 4. Model definition with augmentation and custom CNN\n",
    "    aug = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip('horizontal'),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1)\n",
    "    ])\n",
    "\n",
    "    def build_custom_cnn(input_shape, num_classes):\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = aug(inputs)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        outputs = tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "        return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model = build_custom_cnn((IMAGE_SIZE, IMAGE_SIZE, 3), len(class_names))\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# 5. Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)\n",
    "]\n",
    "\n",
    "# 6. Train head\n",
    "steps_per_epoch = len(train_paths) // GLOBAL_BATCH_SIZE\n",
    "history_head = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS_HEAD,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=len(test_paths) // GLOBAL_BATCH_SIZE,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Final evaluation on CPU\n",
    "y_preds = []\n",
    "y_true = []\n",
    "for imgs, lbls in val_ds.take(len(test_paths) // GLOBAL_BATCH_SIZE + 1):\n",
    "    preds = np.argmax(model.predict(imgs), axis=1)\n",
    "    y_preds.extend(preds)\n",
    "    y_true.extend(lbls.numpy())\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=class_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
