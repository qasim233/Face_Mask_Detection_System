{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2Eisf7svru",
        "outputId": "9f7c2713-6235-40de-f2e2-b60b136cb5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/face-mask-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"omkargurav/face-mask-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6K-8pICt67o",
        "outputId": "559a1631-b955-4759-8d55-2a5d28140da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs detected: 1\n",
            "Epoch 1/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 329ms/step - accuracy: 0.7540 - loss: 0.5164 - val_accuracy: 0.8152 - val_loss: 0.3857 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 190ms/step - accuracy: 0.8391 - loss: 0.3458 - val_accuracy: 0.8198 - val_loss: 0.3651 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 185ms/step - accuracy: 0.8709 - loss: 0.2928 - val_accuracy: 0.7985 - val_loss: 0.4210 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 189ms/step - accuracy: 0.8780 - loss: 0.2846 - val_accuracy: 0.8378 - val_loss: 0.3422 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 184ms/step - accuracy: 0.8685 - loss: 0.2859 - val_accuracy: 0.8072 - val_loss: 0.3833 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 200ms/step - accuracy: 0.8701 - loss: 0.2983 - val_accuracy: 0.7992 - val_loss: 0.4109 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 219ms/step - accuracy: 0.8817 - loss: 0.2659 - val_accuracy: 0.8464 - val_loss: 0.3230 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 215ms/step - accuracy: 0.8813 - loss: 0.2684 - val_accuracy: 0.8384 - val_loss: 0.3424 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 223ms/step - accuracy: 0.8879 - loss: 0.2599 - val_accuracy: 0.7706 - val_loss: 0.4758 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 133ms/step - accuracy: 0.8826 - loss: 0.2787 - val_accuracy: 0.8418 - val_loss: 0.3476 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 222ms/step - accuracy: 0.8939 - loss: 0.2504 - val_accuracy: 0.8172 - val_loss: 0.3724 - learning_rate: 2.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 194ms/step - accuracy: 0.8966 - loss: 0.2499 - val_accuracy: 0.8610 - val_loss: 0.3163 - learning_rate: 2.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 221ms/step - accuracy: 0.8867 - loss: 0.2572 - val_accuracy: 0.8165 - val_loss: 0.3743 - learning_rate: 2.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 219ms/step - accuracy: 0.8948 - loss: 0.2401 - val_accuracy: 0.8218 - val_loss: 0.3654 - learning_rate: 2.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 132ms/step - accuracy: 0.8926 - loss: 0.2508 - val_accuracy: 0.8298 - val_loss: 0.3564 - learning_rate: 2.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 181ms/step - accuracy: 0.8887 - loss: 0.2543 - val_accuracy: 0.8424 - val_loss: 0.3394 - learning_rate: 4.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 219ms/step - accuracy: 0.8928 - loss: 0.2364 - val_accuracy: 0.8431 - val_loss: 0.3353 - learning_rate: 4.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 213ms/step - accuracy: 0.8220 - loss: 0.4090 - val_accuracy: 0.8570 - val_loss: 0.3263 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 220ms/step - accuracy: 0.8787 - loss: 0.2844 - val_accuracy: 0.8577 - val_loss: 0.3157 - learning_rate: 1.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 221ms/step - accuracy: 0.8899 - loss: 0.2556 - val_accuracy: 0.8790 - val_loss: 0.2750 - learning_rate: 1.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 218ms/step - accuracy: 0.9070 - loss: 0.2201 - val_accuracy: 0.8790 - val_loss: 0.2941 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 225ms/step - accuracy: 0.9194 - loss: 0.1984 - val_accuracy: 0.8677 - val_loss: 0.3074 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 185ms/step - accuracy: 0.9146 - loss: 0.1959 - val_accuracy: 0.8717 - val_loss: 0.3106 - learning_rate: 1.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 198ms/step - accuracy: 0.9262 - loss: 0.1694 - val_accuracy: 0.8703 - val_loss: 0.3064 - learning_rate: 2.0000e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 213ms/step - accuracy: 0.9196 - loss: 0.1762 - val_accuracy: 0.8610 - val_loss: 0.3159 - learning_rate: 2.0000e-06\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   with_mask       0.87      0.89      0.88       745\n",
            "without_mask       0.89      0.87      0.88       766\n",
            "\n",
            "    accuracy                           0.88      1511\n",
            "   macro avg       0.88      0.88      0.88      1511\n",
            "weighted avg       0.88      0.88      0.88      1511\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Enable mixed precision for faster GPU training and lower memory usage\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Configure GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Distributed strategy (multi-GPU) if available\n",
        "tf_strategy = tf.distribute.MirroredStrategy()\n",
        "print(f\"GPUs detected: {tf_strategy.num_replicas_in_sync}\")\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = '/kaggle/input/face-mask-dataset/data'\n",
        "IMAGE_SIZE = 224\n",
        "GLOBAL_BATCH_SIZE = 32  # global batch size across all GPUs\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "EPOCHS_HEAD = 20\n",
        "EPOCHS_FINE = 10\n",
        "SEED = 42\n",
        "\n",
        "# 1. Load file paths and labels\n",
        "class_names = sorted(os.listdir(DATA_DIR))\n",
        "paths, labels = [], []\n",
        "for idx, cls in enumerate(class_names):\n",
        "    cls_dir = os.path.join(DATA_DIR, cls)\n",
        "    if os.path.isdir(cls_dir):\n",
        "        cls_paths = glob(os.path.join(cls_dir, '*.*'))\n",
        "        paths += cls_paths\n",
        "        labels += [idx] * len(cls_paths)\n",
        "paths = np.array(paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Shuffle and split\n",
        "i = np.random.RandomState(SEED).permutation(len(paths))\n",
        "paths, labels = paths[i], labels[i]\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    paths, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
        ")\n",
        "\n",
        "# 2. Preprocessing function\n",
        "def preprocess_image(path, label):\n",
        "    img = cv2.imread(path.numpy().decode('utf-8'))\n",
        "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    eq = cv2.equalizeHist(gray)\n",
        "    blur = cv2.GaussianBlur(eq, (3,3), 0)\n",
        "    _, binar = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
        "    closed = cv2.morphologyEx(binar, cv2.MORPH_CLOSE, kernel)\n",
        "    mask = closed.astype(np.float32) / 255.0\n",
        "\n",
        "    mask3 = np.stack([mask]*3, axis=-1)\n",
        "    masked = (rgb * mask3).astype(np.uint8)\n",
        "    return tf.keras.applications.mobilenet_v2.preprocess_input(masked), label\n",
        "\n",
        "# Wrap for tf.data\n",
        "# Wrap for tf.data\n",
        "@tf.function\n",
        "def tf_preprocess(path, label):\n",
        "    img, lbl = tf.py_function(preprocess_image, [path, label], [tf.float32, tf.int64]) # Changed tf.int32 to tf.int64\n",
        "    img.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "    lbl.set_shape([])\n",
        "    return img, lbl\n",
        "\n",
        "# 3. Dataset builder\n",
        "with tf_strategy.scope():\n",
        "    def make_dataset(paths, labels, training=True):\n",
        "        ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "        if training:\n",
        "            ds = ds.shuffle(1000, seed=SEED)\n",
        "        ds = ds.map(tf_preprocess, num_parallel_calls=AUTO)\n",
        "        if training:\n",
        "            ds = ds.repeat()\n",
        "        ds = ds.batch(GLOBAL_BATCH_SIZE)\n",
        "        ds = ds.prefetch(AUTO)\n",
        "        return ds\n",
        "\n",
        "    train_ds = make_dataset(train_paths, train_labels, training=True)\n",
        "    val_ds = make_dataset(test_paths, test_labels, training=False)\n",
        "\n",
        "    # 4. Model definition with augmentation\n",
        "    aug = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip('horizontal'),\n",
        "        tf.keras.layers.RandomRotation(0.1),\n",
        "        tf.keras.layers.RandomZoom(0.1),\n",
        "        tf.keras.layers.RandomContrast(0.1)\n",
        "    ])\n",
        "\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet'\n",
        "    )\n",
        "    base.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "    x = aug(inputs)\n",
        "    x = base(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(len(class_names), activation='softmax', dtype='float32')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# 5. Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)\n",
        "]\n",
        "\n",
        "# 6. Train head\n",
        "steps_per_epoch = len(train_paths) // GLOBAL_BATCH_SIZE\n",
        "history_head = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=len(test_paths) // GLOBAL_BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 7. Fine-tuning last 30% of base\n",
        "with tf_strategy.scope():\n",
        "    total = len(base.layers)\n",
        "    fine_at = int(total * 0.7)\n",
        "    for layer in base.layers[fine_at:]:\n",
        "        layer.trainable = True\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS_HEAD + EPOCHS_FINE,\n",
        "    initial_epoch=history_head.epoch[-1] + 1,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=len(test_paths) // GLOBAL_BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 8. Final evaluation on CPU\n",
        "y_preds = []\n",
        "y_true = []\n",
        "for imgs, lbls in val_ds.take(len(test_paths) // GLOBAL_BATCH_SIZE + 1):\n",
        "    preds = np.argmax(model.predict(imgs), axis=1)\n",
        "    y_preds.extend(preds)\n",
        "    y_true.extend(lbls.numpy())\n",
        "\n",
        "print(classification_report(y_true, y_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9B7F3Jv2gAy",
        "outputId": "891d6392-b8cc-47f7-eb9d-ee0a0b7dcc41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save this model\n",
        "model.save('face_mask_detection.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsIsM2952vPp",
        "outputId": "242e827a-1d01-4cad-e5ff-f8f40f6b1812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/face_mask_detection.h5\n"
          ]
        }
      ],
      "source": [
        "# Check the path of saved model\n",
        "import os\n",
        "current_directory = os.getcwd()\n",
        "model_path = os.path.join(current_directory, 'face_mask_detection.h5')\n",
        "print(model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
